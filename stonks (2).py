# -*- coding: utf-8 -*-
"""Stonks.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xL6z9NH-yl_kgsvqHvWW1asgN-EhRtzm
"""

from google.colab import drive
drive.mount('/content/drive')

import os
base_path = '/content/drive/MyDrive/StockPrediction'
os.makedirs(base_path, exist_ok=True)
os.makedirs(f'{base_path}/data', exist_ok=True)
os.makedirs(f'{base_path}/models', exist_ok=True)
os.makedirs(f'{base_path}/results', exist_ok=True)

!pip install requests pandas numpy matplotlib scikit-learn tensorflow

import numpy as np
import pandas as pd
import requests
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, MultiHeadAttention, LayerNormalization, Conv1D, MaxPooling1D, Flatten
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt

# Function to fetch stock data from Upstox API
def fetch_data(stock_symbol, start_date, interval='week'):
    url = f'https://api.upstox.com/v2/historical-candle/{stock_symbol}/{interval}/{start_date}'
    headers = {'Accept': 'application/json'}
    response = requests.get(url, headers=headers)

    if response.status_code == 200:
        data = response.json()
        # Convert the list of lists to a DataFrame
        df = pd.DataFrame(data['data']['candles'], columns=['timestamp', 'open', 'high', 'low', 'close', 'volume', 'extra'])
        return df  # Return the DataFrame
    else:
        print(f"Error: {response.status_code} - {response.text}")
        return None

def preprocess_data(df):
    """Enhanced preprocessing with additional technical indicators"""
    df['timestamp'] = pd.to_datetime(df['timestamp'])
    df.set_index('timestamp', inplace=True)
    df = df.drop(columns=['extra'])

    # Additional technical indicators
    df['MA_10'] = df['close'].rolling(window=10).mean()
    df['MA_20'] = df['close'].rolling(window=20).mean()
    df['MA_50'] = df['close'].rolling(window=50).mean()

    # Bollinger Bands
    df['BB_middle'] = df['close'].rolling(window=20).mean()
    df['BB_upper'] = df['BB_middle'] + 2 * df['close'].rolling(window=20).std()
    df['BB_lower'] = df['BB_middle'] - 2 * df['close'].rolling(window=20).std()

    # MACD
    exp1 = df['close'].ewm(span=12, adjust=False).mean()
    exp2 = df['close'].ewm(span=26, adjust=False).mean()
    df['MACD'] = exp1 - exp2
    df['Signal_Line'] = df['MACD'].ewm(span=9, adjust=False).mean()

    # RSI with smoothing
    df['RSI'] = compute_rsi(df['close'], window=14)

    # Volume indicators
    df['Volume_MA'] = df['volume'].rolling(window=20).mean()
    df['Volume_Ratio'] = df['volume'] / df['Volume_MA']

    # Price momentum
    df['Price_Momentum'] = df['close'].pct_change(periods=10)

    # Drop NaN values
    df.dropna(inplace=True)

    # Define feature columns
    feature_cols = ['open', 'high', 'low', 'close', 'volume',
                   'MA_10', 'MA_20', 'MA_50',
                   'BB_upper', 'BB_middle', 'BB_lower',
                   'MACD', 'Signal_Line', 'RSI',
                   'Volume_MA', 'Volume_Ratio', 'Price_Momentum']

    # Normalize features
    scaler = MinMaxScaler()
    df[feature_cols] = scaler.fit_transform(df[feature_cols])

    return df, scaler, feature_cols

# RSI Calculation
def compute_rsi(series, window=14):
    delta = series.diff()
    gain = (delta.where(delta > 0, 0)).rolling(window=window).mean()
    loss = (-delta.where(delta < 0, 0)).rolling(window=window).mean()
    rs = gain / loss
    return 100 - (100 / (1 + rs))

def prepare_data(df, feature_cols, target_col, sequence_length=20, prediction_horizon=5):
    """Enhanced data preparation with multiple time steps prediction"""
    X, y = [], []
    for i in range(sequence_length, len(df) - prediction_horizon + 1):
        X.append(df[feature_cols].iloc[i-sequence_length:i].values)
        y.append(df[target_col].iloc[i:i+prediction_horizon].values)
    return np.array(X), np.array(y)

def build_lstm_model(input_shape, output_steps):
    """Enhanced LSTM model with proper input shape"""
    inputs = Input(shape=input_shape)

    x = LSTM(128, return_sequences=True)(inputs)
    x = Dropout(0.3)(x)
    x = LSTM(64, return_sequences=True)(x)
    x = Dropout(0.3)(x)
    x = LSTM(32)(x)
    x = Dropout(0.3)(x)
    x = Dense(64, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(output_steps)(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

def build_transformer_model(input_shape, output_steps, num_heads=8):
    """Enhanced transformer model with additional layers"""
    inputs = Input(shape=input_shape)

    x = Dense(256)(inputs)

    # Multiple transformer blocks
    for _ in range(3):
        attention_output = MultiHeadAttention(num_heads=num_heads, key_dim=32)(x, x)
        x = LayerNormalization(epsilon=1e-6)(x + attention_output)

        # Feed Forward Network
        ffn = Dense(512, activation='relu')(x)
        ffn = Dropout(0.2)(ffn)
        ffn = Dense(256)(ffn)
        x = LayerNormalization(epsilon=1e-6)(x + ffn)

    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(output_steps)(x)

    return Model(inputs=inputs, outputs=outputs)

def build_cnn_model(input_shape, output_steps):
    """
    Build CNN model with proper input shape and output steps

    Args:
        input_shape: Tuple of (sequence_length, n_features)
        output_steps: Number of future time steps to predict
    """
    inputs = Input(shape=input_shape)

    x = Conv1D(filters=64, kernel_size=3, activation='relu')(inputs)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.2)(x)

    x = Conv1D(filters=32, kernel_size=3, activation='relu')(x)
    x = MaxPooling1D(pool_size=2)(x)
    x = Dropout(0.2)(x)

    x = Flatten()(x)
    x = Dense(50, activation='relu')(x)
    x = Dropout(0.2)(x)
    outputs = Dense(output_steps)(x)

    model = Model(inputs=inputs, outputs=outputs)
    return model

# Ensemble Model Predictions
def ensemble_predictions(models, X):
    predictions = [model.predict(X) for model in models]
    ensemble_pred = np.mean(predictions, axis=0)
    return ensemble_pred

def compile_and_train(model, X_train, y_train, X_val, y_val, model_path, epochs=100):
    """Enhanced training with learning rate scheduling and checkpointing"""
    model.compile(
        optimizer=Adam(learning_rate=0.001),
        loss='huber',  # More robust to outliers than MSE
        metrics=['mae', 'mse']
    )

    callbacks = [
        EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),
        ModelCheckpoint(model_path, monitor='val_loss', save_best_only=True)
    ]

    history = model.fit(
        X_train, y_train,
        validation_data=(X_val, y_val),
        epochs=epochs,
        batch_size=32,
        callbacks=callbacks,
        verbose=1
    )
    return history

# Helper function to inverse transform predictions
def inverse_transform_predictions(predictions, scaler, feature_cols, target_col):
    dummy = np.zeros((len(predictions), len(feature_cols)))
    target_idx = feature_cols.index(target_col)
    dummy[:, target_idx] = predictions[:, 0]
    inversed = scaler.inverse_transform(dummy)
    return inversed[:, target_idx]

def plot_predictions(y_actual, predictions, scaler, feature_cols, title):
    """Fixed visualization with proper array handling"""
    plt.figure(figsize=(15, 8))

    # Ensure arrays are 2D
    if len(y_actual.shape) == 1:
        y_actual = y_actual.reshape(-1, 1)
    if len(predictions.shape) == 1:
        predictions = predictions.reshape(-1, 1)

    # Inverse transform the data back to original scale
    def inverse_transform_data(data):
        dummy = np.zeros((len(data), len(feature_cols)))
        target_idx = feature_cols.index('close')
        dummy[:, target_idx] = data.flatten()
        return scaler.inverse_transform(dummy)[:, target_idx]

    # Transform actual values
    y_actual_orig = inverse_transform_data(y_actual)

    # Transform predictions
    pred_orig = inverse_transform_data(predictions)

    # Plot actual values
    plt.plot(y_actual_orig, label='Actual Prices', color='blue', linewidth=2)

    # Plot predictions
    plt.plot(pred_orig, label='Ensemble Predictions', color='orange', linewidth=2)

    # Add confidence interval (using rolling standard deviation)
    window = 5
    rolling_std = pd.Series(pred_orig).rolling(window=window).std()
    plt.fill_between(
        range(len(predictions)),
        pred_orig - rolling_std,
        pred_orig + rolling_std,
        alpha=0.2,
        color='orange',
        label='Prediction Confidence'
    )

    plt.title(title, fontsize=14)
    plt.xlabel('Time', fontsize=12)
    plt.ylabel('Stock Price', fontsize=12)
    plt.legend(fontsize=10)
    plt.grid(True, alpha=0.3)

    return plt.gcf()  # Return the figure

# Main execution
def train_and_evaluate(df, sequence_length=20, prediction_horizon=5):
    """Updated training pipeline with fixed array handling"""
    # Preprocess data
    df_processed, scaler, feature_cols = preprocess_data(df)
    target_col = 'close'

    # Prepare data
    X, y = prepare_data(df_processed, feature_cols, target_col,
                       sequence_length, prediction_horizon)

    # Reshape y to be 2D if it isn't already
    if len(y.shape) == 1:
        y = y.reshape(-1, 1)

    # Split data
    split_idx = int(len(X) * 0.8)
    X_train, X_test = X[:split_idx], X[split_idx:]
    y_train, y_test = y[:split_idx], y[split_idx:]

    # Build and train models
    input_shape = (X_train.shape[1], X_train.shape[2])
    models = [
        build_lstm_model(input_shape, prediction_horizon),
        build_transformer_model(input_shape, prediction_horizon),
        build_cnn_model(input_shape, prediction_horizon)
    ]

    # Train each model
    for i, model in enumerate(models):
        print(f"Training model {i+1}...")
        history = compile_and_train(
            model, X_train, y_train, X_test, y_test,
            f'model_{i}_weights.keras'
        )

    # Generate ensemble predictions
    predictions = np.array([model.predict(X_test) for model in models])
    ensemble_pred = predictions.mean(axis=0)

    # Calculate metrics
    def get_original_scale_metrics(y_true, y_pred):
        # Ensure arrays are 2D
        if len(y_true.shape) == 1:
            y_true = y_true.reshape(-1, 1)
        if len(y_pred.shape) == 1:
            y_pred = y_pred.reshape(-1, 1)

        dummy = np.zeros((len(y_true), len(feature_cols)))
        target_idx = feature_cols.index('close')

        # Transform true values
        dummy[:, target_idx] = y_true.flatten()
        y_true_orig = scaler.inverse_transform(dummy)[:, target_idx]

        # Transform predictions
        dummy[:, target_idx] = y_pred.flatten()
        y_pred_orig = scaler.inverse_transform(dummy)[:, target_idx]

        mae = mean_absolute_error(y_true_orig, y_pred_orig)
        rmse = np.sqrt(mean_squared_error(y_true_orig, y_pred_orig))
        return mae, rmse

    # Reshape arrays for metric calculation
    y_test_reshaped = y_test.reshape(-1, 1)
    ensemble_pred_reshaped = ensemble_pred.reshape(-1, 1)

    mae, rmse = get_original_scale_metrics(y_test_reshaped, ensemble_pred_reshaped)
    print(f"Ensemble MAE: {mae:.2f}")
    print(f"Ensemble RMSE: {rmse:.2f}")

    # Plot results
    fig = plot_predictions(y_test_reshaped, ensemble_pred_reshaped, scaler, feature_cols,
                         'Actual vs Ensemble Predicted Stock Prices')
    plt.show()

    return models, scaler

# Example: Load the preprocessed data
stock_data = fetch_data('NSE_EQ%7CINE002A01018', '2020-01-01')
# Assign the result to 'df' instead of 'stock_data'
df = stock_data  # This line is the fix if df is not None:
print("Data fetched successfully!")
# 2. Train and evaluate the model
print("Training models...")
try:
# Default parameters: sequence_length=20, prediction_horizon=5
  models, scaler = train_and_evaluate(df)
  print("Training completed successfully!")

# 3. Optional: Make predictions for the next few time periods
  print("\nMaking predictions for the next time periods...")

# Prepare the most recent data for prediction
  df_processed, _, feature_cols = preprocess_data(df)
  last_sequence = df_processed[feature_cols].iloc[-20:].values  # Use last 20 time steps
  last_sequence = np.expand_dims(last_sequence, axis=0)  # Add batch dimension

# Get predictions from all models
  predictions = []
  for model in models:
    pred = model.predict(last_sequence)
    predictions.append(pred)

# Calculate ensemble prediction
  ensemble_pred = np.mean(predictions, axis=0)

# Inverse transform the predictions to get actual prices
  dummy = np.zeros((len(ensemble_pred[0]), len(feature_cols)))
  target_idx = feature_cols.index('close')
  dummy[:, target_idx] = ensemble_pred[0]
  actual_prices = scaler.inverse_transform(dummy)[:, target_idx]

  print("\nPredicted prices for next 5 periods:")
  for i, price in enumerate(actual_prices, 1):
    print(f"Period {i}: {price:.2f}")

except Exception as e:
  print(f"An error occurred: {str(e)}")
else:
  print("Failed to fetch data. Please check your stock symbol and API access.")

